---
aliases:
- /ru/python/parsing-html/
changelog:
- 2024-01-28, gpt-4-0125-preview, translated from English
date: 2024-01-29 00:00:28.445820-07:00
description: "\u041F\u0430\u0440\u0441\u0438\u043D\u0433 HTML \u2014 \u044D\u0442\u043E\
  \ \u043F\u0440\u043E\u0446\u0435\u0441\u0441 \u0438\u0437\u0432\u043B\u0435\u0447\
  \u0435\u043D\u0438\u044F \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0438\
  \ \u0438\u0437 \u043A\u043E\u0434\u0430 HTML, \u044D\u0442\u043E \u043A\u0430\u043A\
  \ \u043D\u0430\u0445\u043E\u0434\u0438\u0442\u044C \u0438\u0433\u043E\u043B\u043A\
  \u0438 \u0432 \u0441\u0442\u043E\u0433\u0435 \u0441\u0435\u043D\u0430, \u0435\u0441\
  \u043B\u0438 \u043F\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044C\
  , \u0447\u0442\u043E \u0441\u0442\u043E\u0433 \u0441\u0435\u043D\u0430 \u0441\u043E\
  \u0441\u0442\u043E\u0438\u0442 \u0438\u0437 \u0442\u0435\u0433\u043E\u0432, \u0430\
  \u2026"
lastmod: 2024-02-18 23:08:56.530250
model: gpt-4-0125-preview
summary: "\u041F\u0430\u0440\u0441\u0438\u043D\u0433 HTML \u2014 \u044D\u0442\u043E\
  \ \u043F\u0440\u043E\u0446\u0435\u0441\u0441 \u0438\u0437\u0432\u043B\u0435\u0447\
  \u0435\u043D\u0438\u044F \u0438\u043D\u0444\u043E\u0440\u043C\u0430\u0446\u0438\u0438\
  \ \u0438\u0437 \u043A\u043E\u0434\u0430 HTML, \u044D\u0442\u043E \u043A\u0430\u043A\
  \ \u043D\u0430\u0445\u043E\u0434\u0438\u0442\u044C \u0438\u0433\u043E\u043B\u043A\
  \u0438 \u0432 \u0441\u0442\u043E\u0433\u0435 \u0441\u0435\u043D\u0430, \u0435\u0441\
  \u043B\u0438 \u043F\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044C\
  , \u0447\u0442\u043E \u0441\u0442\u043E\u0433 \u0441\u0435\u043D\u0430 \u0441\u043E\
  \u0441\u0442\u043E\u0438\u0442 \u0438\u0437 \u0442\u0435\u0433\u043E\u0432, \u0430\
  \u2026"
title: "\u0420\u0430\u0437\u0431\u043E\u0440 HTML"
---

{{< edit_this_page >}}

## Что и почему?

Парсинг HTML — это процесс извлечения информации из кода HTML, это как находить иголки в стоге сена, если представить, что стог сена состоит из тегов, а иголки — это данные, которые вам нужны. Программисты делают это для того, чтобы извлекать данные с веб-сайтов, что может быть чем угодно, от заголовков на новостном сайте до цен в интернет-магазине.

## Как это сделать:

Давайте используем Python, чтобы вытащить некоторые данные из образца HTML с помощью библиотеки `BeautifulSoup`, которая делает парсинг легким. Если вы еще не установили этот пакет, сделайте это с помощью `pip install beautifulsoup4`.

```Python
from bs4 import BeautifulSoup

# Представим, это ваш HTML
html_doc = """
<html>
<head>
    <title>История Сони</title>
</head>
<body>
    <p class="title">
        <b>История Сони</b>
    </p>
    <p class="story">Жили-были три маленькие сестрички, и звали их
        <a href="http://example.com/elsie" class="sister" id="link1">Эльзи</a>,
        <a href="http://example.com/lacie" class="sister" id="link2">Лейси</a> и
        <a href="http://example.com/tillie" class="sister" id="link3">Тилли</a>;
        и жили они на дне колодца.</p>
</body>
</html>
"""

# Обработаем через Soup
soup = BeautifulSoup(html_doc, 'html.parser')

# Найдем тег title
title_tag = soup.title
print("Название истории:", title_tag.string)

# Найдем все теги 'a' с классом 'sister'
sister_tags = soup.find_all('a', class_='sister')
print("Имена сестер и URL-адреса:")
for sister in sister_tags:
    print(f"- Имя: {sister.string}, URL: {sister['href']}")
```

Вывод будет следующим:

```
Название истории: История Сони
Имена сестер и URL-адреса:
- Имя: Эльзи, URL: http://example.com/elsie
- Имя: Лейси, URL: http://example.com/lacie
- Имя: Тилли, URL: http://example.com/tillie
```

## Более глубокое погружение

В заре интернета HTML анализировали с помощью регулярных выражений и большой доли надежды. Это было беспорядочно, так как HTML не всегда аккуратен и предсказуем. Тогда на сцену вышли библиотеки вроде BeautifulSoup, которые навигируют по древовидной структуре HTML, предлагая нежный способ разрезать и крошить данные.

Существуют также альтернативы, такие как `lxml` и `html.parser`, которые сам BeautifulSoup может использовать в качестве парсеров. `lxml` работает быстрее, но менее толерантен к плохому HTML, в то время как `html.parser` медленнее, но не заботится о сломанных тегах.

Под капотом эти библиотеки строят дерево разбора, превращая теги в объекты, с которыми можно взаимодействовать. BeautifulSoup похож на дружелюбный интерфейс к этим парсерам, переводя ваши вопросы — вроде "Какой заголовок?" или "Есть ли здесь ссылки?" — в действия на дереве.

## См. также

- Документация BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- Введение в анализ HTML с помощью регулярных выражений (и почему этого не стоит делать): https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags
- Веб-скрапинг с Python (практическое руководство): https://realpython.com/beautiful-soup-web-scraper-python/
