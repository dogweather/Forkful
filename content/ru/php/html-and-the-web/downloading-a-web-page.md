---
changelog:
- 2024-01-28, gpt-4-0125-preview, translated from English
date: 2024-01-28 23:57:58.466139-07:00
description: "\u0417\u0430\u0433\u0440\u0443\u0437\u043A\u0430 \u0432\u0435\u0431\
  -\u0441\u0442\u0440\u0430\u043D\u0438\u0446\u044B \u043E\u0437\u043D\u0430\u0447\
  \u0430\u0435\u0442 \u043F\u043E\u043B\u0443\u0447\u0435\u043D\u0438\u0435 \u043E\
  \u043D\u043B\u0430\u0439\u043D-\u043A\u043E\u043D\u0442\u0435\u043D\u0442\u0430\
  \ \u0434\u043B\u044F \u0435\u0433\u043E \u0438\u0441\u043F\u043E\u043B\u044C\u0437\
  \u043E\u0432\u0430\u043D\u0438\u044F \u0438\u043B\u0438 \u0430\u043D\u0430\u043B\
  \u0438\u0437\u0430 \u0432 \u043E\u0444\u043B\u0430\u0439\u043D\u0435. \u041F\u0440\
  \u043E\u0433\u0440\u0430\u043C\u043C\u0438\u0441\u0442\u044B \u0434\u0435\u043B\u0430\
  \u044E\u0442 \u044D\u0442\u043E \u0434\u043B\u044F \u0432\u0435\u0431-\u0441\u043A\
  \u0440\u0430\u043F\u0438\u043D\u0433\u0430, \u0430\u043D\u0430\u043B\u0438\u0437\
  \u0430\u2026"
lastmod: '2024-03-13T22:44:45.207959-06:00'
model: gpt-4-0125-preview
summary: "\u0417\u0430\u0433\u0440\u0443\u0437\u043A\u0430 \u0432\u0435\u0431-\u0441\
  \u0442\u0440\u0430\u043D\u0438\u0446\u044B \u043E\u0437\u043D\u0430\u0447\u0430\u0435\
  \u0442 \u043F\u043E\u043B\u0443\u0447\u0435\u043D\u0438\u0435 \u043E\u043D\u043B\
  \u0430\u0439\u043D-\u043A\u043E\u043D\u0442\u0435\u043D\u0442\u0430 \u0434\u043B\
  \u044F \u0435\u0433\u043E \u0438\u0441\u043F\u043E\u043B\u044C\u0437\u043E\u0432\
  \u0430\u043D\u0438\u044F \u0438\u043B\u0438 \u0430\u043D\u0430\u043B\u0438\u0437\
  \u0430 \u0432 \u043E\u0444\u043B\u0430\u0439\u043D\u0435."
title: "\u0417\u0430\u0433\u0440\u0443\u0437\u043A\u0430 \u0432\u0435\u0431-\u0441\
  \u0442\u0440\u0430\u043D\u0438\u0446\u044B"
weight: 42
---

## Как это сделать:
PHP делает загрузку веб-страниц довольно простой. Вот простой пример использования `file_get_contents()`:

```php
<?php
$url = "http://example.com";
$pageContent = file_get_contents($url);

if ($pageContent !== false) {
    echo "Страница успешно загружена.\n";
    // Работаем с $pageContent
} else {
    echo "Не удалось загрузить страницу.\n";
}
?>
```

И если вам нужен больший контроль или хотите управлять HTTP-заголовками, куками или POST-запросами, вы можете стать продвинутым пользователем с `cURL`:

```php
<?php
$url = "http://example.com";
$ch = curl_init($url);

curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
$pageContent = curl_exec($ch);

if (curl_errno($ch)) {
    echo "Ошибка: " . curl_error($ch) . "\n";
} else {
    echo "Страница успешно загружена.\n";
    // Работаем с $pageContent
}

curl_close($ch);
?>
```

Пример вывода может быть таким:
```
Страница успешно загружена.
```

## Глубокое погружение
Загрузка веб-страниц — практика столь же старая, как и сам веб. Изначально, для взаимодействия со страницами использовались инструменты командной строки, типа `wget` или `curl`. Однако, по мере развития PHP, функции сделали эти задачи выполнимыми внутри скриптов.

Сравним:

- `file_get_contents()`: Прост в использовании для базовых задач, но не имеет продвинутых функций. Хорош для быстрых получений без лишних проблем.
- `cURL`: Швейцарский нож для веб-запросов в PHP. Управляет сложными сценариями, такими как аутентификация, куки и установка заголовков. Немного громоздкий, но есть, когда вам нужна дополнительная мощность.

В закулисье, `file_get_contents()` отправляет стандартный GET-запрос. Это означает, что он действует так же, как браузер, когда вы вводите URL. Но без контекста HTTP (такого как заголовки), некоторые страницы могут не возвращать правильное содержимое.

`cURL`, с другой стороны, может имитировать поведение браузера в полной мере. Это необходимо для капризных страниц, которые ожидают определенные заголовки или куки.

Помните, некоторые сайты не одобряют скрапинг. Всегда уважайте `robots.txt` и условия использования.

## Смотрите также
- [Руководство PHP по file_get_contents()](http://php.net/manual/en/function.file-get-contents.php)
- [Руководство PHP по cURL](http://php.net/manual/en/book.curl.php)
- [Спецификации robots.txt](https://developers.google.com/search/docs/advanced/robots/robots_txt)
